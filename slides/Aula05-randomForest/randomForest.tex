%
% Problemas para identificação de técnicas
% de representação de conhecimento
%
% by Fabrício Jailson Barth 2006
%

\documentclass[landscape,pdftex]{jomislides}

\slidesmag{5} % escala, qto maior maiores serão as letras/figras/etc.

%\centerslidesfalse

\usepackage{algorithmic}
\usepackage{alltt}
\usepackage{booktabs}
\usepackage{algorithm}

%
% Slides
% ======
%


\begin{document}

%\input{autorHeaders}

\title{Random Forest} 
\author{Fabrício Barth}
\institution{}
\date{Agosto de 2019}

\SlideHeader{}
            {}
\SlideFooter{\theslidepartheading $\;$ --- $\;$ \theslideheading}
            {\theslide}

\vpagecolor[white]{white}


\subtitle{}

\maketitle

\begin{Slide}{Ensemble Learning}
\begin{itemize}
	\item Métodos que geram diversos modelos e agregam o seu resultado.
	\item No caso do Random Forest, são geradas diversas árvores e cada árvore é gerada considerando apenas um sub-conjunto do conjunto de treinamento.
\end{itemize}
\end{Slide}

\begin{Slide}{Random Forest}
\begin{itemize}
	\item O algoritmo possui apenas dois parâmetros configuráveis: 
	\begin{itemize}
		\item quantidade de atributos considerados em cada árvore ($m_{try}$), e;
		\item quantidade de árvores ($n_{tree}$).
	\end{itemize}
\end{itemize}
\end{Slide}

\begin{Slide}{Random Forest}
Para problemas de classificação e regressão o algoritmo funciona da seguinte forma:
\begin{itemize}
	\item Cria $n_{tree}$ sub-conjuntos de exemplos a partir do dataset original.
	\item Para cada sub-conjunto de exemplos cria-se uma árvore de classificação ou regressão sem poda. 
	A criação de cada árvore considera apenas um sub-conjunto de exemplos: $m_{try}$ atributos selecionados aleatoriamente e $2/3$ dos exemplos também selecionados aleatoriamente. 
	
	\newpage
	
	\item A predição para novos dados acontece pela agregação das predições das $n_{tree}$ árvores.
	\item Para problemas de \textbf{classificação} é considerado a maioria dos votos.
	\item Para problemas de \textbf{regressão} é considerado a média dos votos.  
	
\end{itemize}	
\end{Slide}

\begin{Slide}{Particularidades de implementação no sklearn}
	\scriptsize
	\begin{alltt}
	max\_features : int, float, string or None, optional (default="auto")
	The number of features to consider when looking for the best split:
	
	If int, then consider max\_features features at each split.
	If float, then max\_features is a fraction and int(max_features * n\_features) features are considered at each split.
	If "auto", then max\_features=sqrt(n\_features).
	If "sqrt", then max\_features=sqrt(n\_features) (same as ?auto?).
	If "log2", then max\_features=log2(n\_features).
	If None, then max\_features=n\_features.
\end{alltt}

\newpage

\begin{alltt}
	max\_depth : integer or None, optional (default=None)
	The maximum depth of the tree. If None, then nodes are expanded 
	until all leaves are pure or until all leaves contain less than
	min\_samples\_split samples.
	
	warm\_start : bool, optional (default=False)
	When set to True, reuse the solution of the previous call to fit 
	and add more estimators to the ensemble, otherwise, just fit a 
	whole new forest.
\end{alltt}

\end{Slide}

\begin{Slide}{Estimativa de erro}
	\begin{itemize}
		\item Uma estimativa de erro, usando apenas o conjunto de treinamento, pode ser obtida através do conjunto de treinamento. Ao invés de ser utilizado algum outro método, como \textit{cross-validation}.
		\item Para cada árvore construída é usado um sub-conjunto de exemplos. $1/3$ dos exemplos são mantidos fora do conjunto de treinamento. Estes exemplos mantidos fora do conjunto de treinamento são utilizados como teste.
	\end{itemize}
\end{Slide}

\begin{Slide}{Exemplos}
	
	https://github.com/fbarth/ml-espm/blob/master/scripts/python/05\_01\_random\_forest.ipynb
	
\end{Slide}


\begin{Slide}{Hiper-parâmetros}
	\begin{itemize}
		\item n\_estimators = number of trees in the foreset
		\item max\_features = max number of features considered for splitting a node
		\item max\_depth = max number of levels in each decision tree
		\item min\_samples\_split = min number of data points placed in a node before the node is split
		\item min\_samples\_leaf = min number of data points allowed in a leaf node
		\item bootstrap = method for sampling data points (with or without replacement)
	\end{itemize}
\end{Slide}


\begin{Slide}{Material de \textbf{consulta}}
  \begin{itemize}
  \item Liaw and Wiener. Classification and Regression by randomForest. R News 2 (3): 18--22 (2002)
  \item Breiman and Cutler. Random Forests. Acessado em https://www.stat.berkeley.edu/~breiman/RandomForests/
  \item http://rpubs.com/fbarth/exemploRandomForest
  \item https://scikit-learn.org/stable/modules/generated/
  sklearn.ensemble.RandomForestClassifier.html
  \end{itemize}
\end{Slide}


\end{document}

