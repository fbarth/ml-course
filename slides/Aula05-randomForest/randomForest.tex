%
% Problemas para identificação de técnicas
% de representação de conhecimento
%
% by Fabrício Jailson Barth 2006
%

\documentclass[landscape,pdftex]{jomislides}

\slidesmag{5} % escala, qto maior maiores serão as letras/figras/etc.

%\centerslidesfalse

\usepackage{algorithmic}
\usepackage{alltt}
\usepackage{booktabs}
\usepackage{algorithm}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}

\usepackage[procnames]{listings}
\lstset{language=Python, 
	basicstyle=\ttfamily\small, 
	keywordstyle=\color{keywords},
	commentstyle=\color{comments},
	stringstyle=\color{red},
	showstringspaces=false,
	identifierstyle=\color{green},
	procnamekeys={def,class}}

%
% Slides
% ======
%


\begin{document}

%\input{autorHeaders}

\title{Random Forest} 
\author{Fabrício Barth}
\institution{}
\date{Agosto de 2019}

\SlideHeader{}
            {}
\SlideFooter{\theslidepartheading $\;$ --- $\;$ \theslideheading}
            {\theslide}

\vpagecolor[white]{white}


\subtitle{}

\maketitle

\begin{Slide}{Ensemble Learning}
\begin{itemize}
	\item Métodos que geram diversos modelos e agregam o seu resultado.
	\item No caso do Random Forest, são geradas diversas árvores e cada árvore é gerada considerando apenas um sub-conjunto do conjunto de treinamento.
	\item Este tipo de algoritmo também é chamado de \textit{Bootstrap Aggregating} ou \textbf{\textit{Bagging}}.
\end{itemize}
\end{Slide}

\begin{Slide}{Random Forest}
\begin{itemize}
	\item O algoritmo possui apenas dois parâmetros configuráveis: 
	\begin{itemize}
		\item quantidade de atributos considerados em cada árvore ($m_{try}$), e;
		\item quantidade de árvores ($n_{tree}$).
	\end{itemize}
\end{itemize}
\end{Slide}

\begin{Slide}{Random Forest}
Para problemas de classificação e regressão o algoritmo funciona da seguinte forma:
\begin{itemize}
	\item Cria $n_{tree}$ sub-conjuntos de exemplos a partir do dataset original.
	\item Para cada sub-conjunto de exemplos cria-se uma árvore de classificação ou regressão sem poda. 
	A criação de cada árvore considera apenas um sub-conjunto de exemplos: $m_{try}$ atributos selecionados aleatoriamente e $2/3$ dos exemplos também selecionados aleatoriamente. 
	
	\newpage
	
	\item A predição para novos dados acontece pela agregação das predições das $n_{tree}$ árvores.
	\item Para problemas de \textbf{classificação} é considerado a maioria dos votos.
	\item Para problemas de \textbf{regressão} é considerado a média dos votos.  
	
\end{itemize}	
\end{Slide}

\begin{Slide}{Particularidades de implementação no sklearn}
	\scriptsize
	\begin{alltt}
	max\_features : int, float, string or None, optional (default="auto")
	The number of features to consider when looking for the best split:
	
	If int, then consider max\_features features at each split.
	If float, then max\_features is a fraction and int(max_features * n\_features) features are considered at each split.
	If "auto", then max\_features=sqrt(n\_features).
	If "sqrt", then max\_features=sqrt(n\_features) (same as ?auto?).
	If "log2", then max\_features=log2(n\_features).
	If None, then max\_features=n\_features.
\end{alltt}

\newpage

\begin{alltt}
	max\_depth : integer or None, optional (default=None)
	The maximum depth of the tree. If None, then nodes are expanded 
	until all leaves are pure or until all leaves contain less than
	min\_samples\_split samples.
	
	warm\_start : bool, optional (default=False)
	When set to True, reuse the solution of the previous call to fit 
	and add more estimators to the ensemble, otherwise, just fit a 
	whole new forest.
\end{alltt}

\end{Slide}

\begin{Slide}{Estimativa de erro}
	\begin{itemize}
		\item Uma estimativa de erro, usando apenas o conjunto de treinamento, pode ser obtida através do conjunto de treinamento. Ao invés de ser utilizado algum outro método, como \textit{cross-validation}.
		\item Para cada árvore construída é usado um sub-conjunto de exemplos. $1/3$ dos exemplos são mantidos fora do conjunto de treinamento. Estes exemplos mantidos fora do conjunto de treinamento são utilizados como teste.
	\end{itemize}
\end{Slide}

\begin{Slide}{Exemplos}
	
	https://github.com/fbarth/ml-espm/blob/master/scripts/python/05\_01\_random\_forest.ipynb
	
\end{Slide}


\begin{Slide}{Hiperparâmetros}
	
	Um modelo de Random Forest tem os seguintes hiperparâmetros: 
	
	\begin{itemize}
		\item n\_estimators = número de árvores na floresta. 
		\item max\_features = número máximo de atributos considerados na seleção de um atributo.
		\item max\_depth = número máximo de níveis em cada árvore de decisão.
		
		\newpage
		
		\item min\_samples\_split = número mínimo de exemplos que devem ser considerados antes de cada divisão de nodo.
		\item min\_samples\_leaf = número mínimo de exemplos em cada nodo final.
		\item bootstrap = método para amostragem de exemplos (com ou sem \textit{replacement})
	\end{itemize}
\end{Slide}

\begin{Slide}{GridSearch}
\tiny
\begin{lstlisting}
from sklearn.model_selection import GridSearchCV
param_grid = { 
 'n_estimators': [100, 200, 500, 600, 800, 1000],
 'max_features': ['auto', 'sqrt', 'log2'],
 'max_depth' : [5,10,50,100,150,None]
}
rfc=RandomForestClassifier(random_state=4)
CV_rfc = GridSearchCV(
 estimator=rfc, param_grid=param_grid, 
 cv= 3, verbose=1, n_jobs=4)
CV_rfc.fit(X_train, y_train)
\end{lstlisting}
\end{Slide}

\begin{Slide}{GridSearch}
	\begin{itemize}
		\item Executa todas as combinações considerando todos os valores de todas as variáveis do \textit{grid}.
		\item No caso do exemplo anterior, $6 \times 3 \times 6$, que gera 108 modelos possíveis. 
		\item Além disso, cada modelo é gerado 3 vezes por que o \textit{GridSearchCV} executa uma rotina de cross-validation igual a 3.
	\end{itemize}
\end{Slide}

\begin{Slide}{GridSearch}
\begin{figure}[htbp]
	\centering 
	\resizebox*{1\columnwidth}{0.8\textheight}
	{\includegraphics{figuras/grid_search}}
\end{figure}
\end{Slide}


\begin{Slide}{GridSearch e Random SearchCV}
	\begin{itemize}
		\item Apesar do processo demorar, o GridSearch retorna a melhor configuração para os parâmetros testados. 
		
		\item Ao invés de testar todas as possibilidades, pode-se utilizar o Random SearchCV para testar apenas parte das configurações de forma aleatória.
		
		\item o Random SearchCV testa no máximo $n$ combinações, onde $n$ é determinado pelo parâmetro $n\_iter$.
		
	\end{itemize}
\end{Slide}

\begin{Slide}{Exemplo de código}
	https://github.com/fbarth/ml-espm/blob/master/scripts/python/05\_02\_random\_forest.ipynb
\end{Slide}

\begin{Slide}{Considerações finais}
	\begin{itemize}
		\item Em comparação com as árvores de decisão, o que se perde é a estrutura simples e interpretável; o que se ganha é o aumento da precisão. 
	\end{itemize}
	
\end{Slide}

\begin{Slide}{Material de \textbf{consulta}}
  \begin{itemize}
  	
  	 \item Breiman and Cutler. Random Forests. Acessado em https://www.stat.berkeley.edu/~breiman/RandomForests/
  	 
  \item Liaw and Wiener. Classification and Regression by randomForest. R News 2 (3): 18--22 (2002)
  
  \newpage
  
  \item https://scikit-learn.org/stable/modules/generated/
  sklearn.ensemble.RandomForestClassifier.html
  
  \item https://scikit-learn.org/stable/modules/generated/
  sklearn.model\_selection.GridSearchCV.html
  
  \item https://scikit-learn.org/stable/modules/generated/
  sklearn.model\_selection.RandomizedSearchCV.html
 
  \item http://rpubs.com/fbarth/exemploRandomForest
  
  \end{itemize}
\end{Slide}


\end{document}

